import streamlit as st
import os
import logging
import json
import PyPDF2
from sentence_transformers import SentenceTransformer
import faiss
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import tiktoken
from transformers import pipeline
import google.generativeai as genai

# Set up logging
logging.basicConfig(level=logging.INFO)

# Load environment variables
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Validate API Key
if not GEMINI_API_KEY:
    logging.error("Gemini API key is missing.")
    raise ValueError("Gemini API key is missing. Please check your environment variables.")
else:
    logging.info("Gemini API key loaded successfully.")

genai.configure(api_key=GEMINI_API_KEY)
model_name = "gemini-1.5-flash"
model = genai.GenerativeModel(model_name)

# Define llm.invoke to use Google's Generative AI
class GoogleLLMWrapper:
    def invoke(self, prompt):
        try:
            response = model.generate_content(prompt)
            if response and hasattr(response, "text"):  # Check if response is valid
                return {"content": response.text}  # Return the text content
            else:
                logging.error("Invalid response from Gemini model.")
                return None
        except Exception as e:
            logging.error(f"Error invoking Gemini model: {e}")
            return None

# Initialize llm as the wrapper
llm = GoogleLLMWrapper()

# Token Counting Function
def count_tokens(text):
    """Count the number of tokens in the input text."""
    enc = tiktoken.get_encoding("cl100k_base")
    return len(enc.encode(text))

# Safe Summarization Function
def safe_summarize(content, token_limit=2000):
    """Summarize content to stay within token limits."""
    if count_tokens(content) > token_limit:
        summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
        return summarizer(content, max_length=300, min_length=50, do_sample=False)[0]['summary_text']
    return content

# Extract Text from PDFs
def extract_text_from_pdfs(pdf_paths):
    text = ""
    for pdf_path in pdf_paths:
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                for page in reader.pages:
                    text += page.extract_text() or ""  # Handle empty text
        except Exception as e:
            logging.error(f"Error extracting text from {pdf_path}: {e}")
    return text

# Generate Embeddings
def generate_embeddings(text):
    """Generate embeddings for the text using SentenceTransformer."""
    model = SentenceTransformer('all-MiniLM-L6-v2')
    chunks = split_text_into_chunks(text)
    embeddings = model.encode(chunks)
    return chunks, embeddings

# Create FAISS Index
def create_faiss_index(embeddings):
    if embeddings.size == 0:
        logging.error("No embeddings provided to create FAISS index.")
        return None
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    return index

# Split Text into Chunks
def split_text_into_chunks(text, chunk_size=1000, chunk_overlap=200):
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)
    return chunks

# Search for Relevant Content in PDFs
def get_top_chunks(query, index, pdf_chunks, top_k=5):
    """Search for the most relevant content in PDF using FAISS."""
    model = SentenceTransformer('all-MiniLM-L6-v2')
    query_embedding = model.encode([query])
    D, I = index.search(query_embedding, k=top_k)
    top_chunks = [pdf_chunks[i] for i in I[0]] if I[0].size > 0 else []
    return top_chunks

# Dynamic Prompt Builder
def build_dynamic_prompt(context, description, num_chunks):
    prompt=ChatPromptTemplate.from_template(
        """
        You are a medical expert. Analyze the following skin issue based on {num_chunks} retrieved medical references.
        Context: {context}
        User Description: {description}

        Provide the top 5 possible diseases in JSON format:
        [
            {{"disease": "<Disease Name 1>", "score": <Possibility Score 1>}},
            ...
        ]
        """
    )
    return prompt

# Generate Explanations for Predictions
def generate_explanations(predictions, retrieved_chunks):
    explanations = {}
    for disease in predictions:
        explanations[disease["disease"]] = {
            "score": disease["score"],
            "context": retrieved_chunks
        }
    return explanations

# Generate Diagnostic Questions
def generate_questions(disease_list, llm, max_questions=15):
    """
    Generate diagnostic questions tailored to the predicted diseases using the LLM.
    """
    # Prepare input for the LLM
    prompt = f"""
    You are a medical expert. The following diseases are possible based on user input: {', '.join(disease_list)}.
    Generate up to {max_questions} diagnostic questions that help distinguish between these diseases.
    Focus on unique symptoms or characteristics for each disease. 
    For example, if "rash" and "burn" are among the diseases, ask about pain, itching, etc., which are distinguishing features.

    Output the questions in a JSON list format, e.g.:
    [
        "Question 1",
        "Question 2",
        ...
    ]
    """
    # Use the LLM to generate questions
    response = llm.invoke(prompt)
    questions_json = response.content.strip()
    logging.info(f"Questions JSON: {questions_json}")
    try:
        questions = json.loads(questions_json)
        if not isinstance(questions, list):
            raise ValueError("Expected a list of questions.")
    except (json.JSONDecodeError, ValueError) as e:
        logging.error(f"Failed to parse questions JSON: {e}")
        questions = []
    return questions[:max_questions]


# Refine Disease Prediction
import json


def refine_disease_prediction(disease_list, user_answers, llm):
    """
    Refine the predicted disease based on user answers using the LLM.
    """
    # Prepare input for the LLM
    prompt = f"""
    You are a medical expert. Based on the user's symptoms, the following diseases were predicted: {', '.join(disease_list)}.
    The user has answered diagnostic questions as follows:
    {user_answers}

    Use this information to refine the predictions and provide the most likely disease. 
    Output the final disease in this JSON format:
    [
        "disease": "<Final Disease>",
        "justification": "<Reason based on answers>"
    ]
    """
    # Use the LLM to refine predictions
    response = llm.invoke(prompt)
    if not response or not hasattr(response, "content") or not response.content.strip():
        logging.error("Invalid or empty response from LLM.")
        st.error("Unable to process diagnosis. Please try again.")
        return
    refined_disease_str = response.content.strip()
    logging.info(f"Response from LLM: {refined_disease_str}")
    try:
        refined_disease = json.loads(refined_disease_str)
    except json.JSONDecodeError as e:
        logging.error(f"Failed to parse JSON response: {e}")
        refined_disease = {"disease": "Unknown", "justification": "Unable to parse response."}
    # Ensure the response contains the expected keys
    if not isinstance(refined_disease, dict) or "disease" not in refined_disease or "justification" not in refined_disease:
        refined_disease = {"disease": "Unknown", "justification": "Invalid response format."}
    return refined_disease


# Streamlit Interface
st.title("SkinGenie - Skin Disease Diagnosis Assistant")
st.markdown("""
    ðŸ§¬ **Diagnose Skin Diseases**: Upload an image, provide a description of the skin problem, and receive guidance on possible diseases along with diagnostic questions for refinement.
""")

# User Inputs
uploaded_image = st.file_uploader("Upload an image of the skin issue (optional):", type=["jpg", "jpeg", "png"])
user_description = st.text_area("Describe your skin problem (e.g., 'Red rash with itchiness for 3 days'):")
if not user_description or len(user_description.strip()) < 5:
    st.error("Please provide a detailed description of your skin problem.")
    st.stop()  # Stop further processing if input is invalid
# Process Input
if user_description:
    try:
        # Extract and Process PDF Data
        pdf_paths = ["skin_diseases_guide1.pdf", "skin_diseases_guide2.pdf"]
        
        def clean_pdf_text(text):
            """Remove irrelevant sections like acknowledgments, prefaces, and exam questions."""
            filtered_text = []
            for line in text.split("\n"):
                if any(keyword in line for keyword in ["ACKNOWLEDGEMENT", "PREFACE", "Table", "exam"]):
                    continue
                filtered_text.append(line.strip())
            return " ".join(filtered_text)

        pdf_text = extract_text_from_pdfs(pdf_paths)
        logging.info("PDF text extracted successfully.\n")
        pdf_text = clean_pdf_text(pdf_text)
        logging.info("PDF text cleaned successfully.\n")
        pdf_chunks, pdf_embeddings = generate_embeddings(pdf_text)
        logging.info("Embeddings generated successfully.\n")
        pdf_index = create_faiss_index(pdf_embeddings)
        logging.info("FAISS index created successfully.\n")


        if not pdf_index:
            st.error("Failed to create FAISS index. Ensure embeddings were generated correctly.")
            st.stop()

        # Search Relevant Content
        pdf_search_results = get_top_chunks(user_description, pdf_index, pdf_chunks, top_k=3)
        context = safe_summarize(" ".join(pdf_search_results), token_limit=3000)

        # Calculate the number of chunks used in the context
        num_chunks = min(len(pdf_search_results), 5)

        # Build and Query Prompt
        prompt = build_dynamic_prompt(context, user_description, num_chunks)
        query_context = {"context": context, "description": user_description, "num_chunks": num_chunks}  # Include num_chunks
        formatted_prompt = prompt.format_prompt(**query_context).to_string()
        logging.info(f"Final Prompt for LLM:\n{formatted_prompt}")


        
        response = llm.invoke(formatted_prompt)
        if not response or not hasattr(response, "content") or not response.content.strip():
           logging.error("Invalid or empty response from LLM.")
           st.error("Unable to process diagnosis. Please try again.")
           

        raw_response = response.content.strip()
        logging.info(f"Raw LLM response \n : {raw_response}. \n")

        # diseases_with_scores_str = getattr(response, "content", "[]")
        # logging.info(f"Diseases with scores: {diseases_with_scores_str}")

        # Parse and handle diseases_with_scores
        try:
            diseases_with_scores_str = getattr(response, "content", "[]")
            logging.info(f"Diseases with scores: {diseases_with_scores_str}")
            diseases_with_scores = json.loads(diseases_with_scores_str)
            if not isinstance(diseases_with_scores, list):
                raise ValueError("Expected a list of dictionaries.")
        except (json.JSONDecodeError, ValueError) as e:
            logging.error(f"Failed to parse diseases JSON: {e}")
            diseases_with_scores = []

        # Extract disease names
        disease_names = [d.get("disease", "Unknown") for d in diseases_with_scores if isinstance(d, dict)]

        st.subheader("Top 5 Possible Diseases:")
        st.write(diseases_with_scores)

        # Integrate Image Model Predictions
        if uploaded_image:
            st.info("Processing the uploaded image to identify diseases...")
            # Placeholder for image model integration
            image_model_predictions = ["DiseaseX", "DiseaseY"]
            disease_names.extend(image_model_predictions)
            st.write("Diseases from image analysis:", image_model_predictions)

        # Generate Diagnostic Questions
        # Generate and Display Diagnostic Questions
        st.subheader("Diagnostic Questions:")
        questions = generate_questions(disease_names, llm, max_questions=15)
        user_answers = {}
        for i, question in enumerate(questions[:15], 1):  # Limit to 15 questions
            answer = st.text_input(f"Question {i}: {question}", key=f"answer_{i}")
            user_answers[question] = answer or "No answer provided"  # Default value

        # Refine Disease Prediction Based on User Answers
        if st.button("Refine Diagnosis"):
            refined_disease = refine_disease_prediction(disease_names, user_answers, llm)
            st.subheader("Final Disease Prediction")
            st.write(f"**Disease:** {refined_disease['disease']}")
            st.write(f"**Justification:** {refined_disease['justification']}")

            # Provide Advice
            st.subheader("Advice:")
            advice = f"Follow appropriate steps for {refined_disease['disease']}. Consult a dermatologist for further guidance."
            st.write(advice)

    except Exception as e:  
        logging.error(f"Error during diagnosis: {e}")
        st.error("An error occurred. Please try again.")
