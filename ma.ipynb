{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/activations_tf.py:22\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:38\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/activations_tf.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m         )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[0;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/integrations/integration_utils.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TFPreTrainedModel\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_groq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/__init__.py:14\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[1;32m     11\u001b[0m     export_optimized_onnx_model,\n\u001b[1;32m     12\u001b[0m     export_static_quantized_openvino_model,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchEncoding\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/evaluation/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMSEEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MSEEvaluator\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMSEEvaluatorFromDataFrame\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MSEEvaluatorFromDataFrame\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNanoBEIREvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NanoBEIREvaluator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParaphraseMiningEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParaphraseMiningEvaluator\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mRerankingEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RerankingEvaluator\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/evaluation/NanoBEIREvaluator.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mInformationRetrievalEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InformationRetrievalEvaluator\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_torch_npu_available\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module, get_relative_import_files\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerModelCardData, generate_model_card\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimilarity_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimilarityFunction\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __MODEL_HUB_ORGANIZATION__, __version__\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/model_card.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainerCallback\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CodeCarbonCallback\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelcard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_markdown_table\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_callback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainerControl, TrainerState\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_groq import ChatGroq\n",
    "import faiss\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "from geopy.geocoders import Nominatim\n",
    "import googlemaps\n",
    "from streamlit_folium import st_folium\n",
    "import folium\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Additional imports for image processing\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GOOGLE_PLACES_API_KEY = os.getenv(\"GOOGLE_PLACES_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Validate API Keys\n",
    "if not GROQ_API_KEY or not GOOGLE_PLACES_API_KEY or not GEMINI_API_KEY:\n",
    "    st.error(\"API keys for Groq, Google Places, and/or Gemini are missing. Check your environment variables.\")\n",
    "    st.stop()\n",
    "\n",
    "# Initialize APIs\n",
    "llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=\"llama3-8b-8192\")\n",
    "gmaps = googlemaps.Client(key=GOOGLE_PLACES_API_KEY)\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model_name = \"gemini-1.5-flash\"\n",
    "model = genai.GenerativeModel(model_name)\n",
    "\n",
    "# Define llm.invoke to use Google's Generative AI\n",
    "class GoogleLLMWrapper:\n",
    "    def invoke(self, prompt):\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            if response and hasattr(response, \"text\"):  # Check if response is valid\n",
    "                return {\"content\": response.text}  # Return the text content\n",
    "            else:\n",
    "                logging.error(\"Invalid response from Gemini model.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error invoking Gemini model: {e}\")\n",
    "            return None\n",
    "\n",
    "llm = GoogleLLMWrapper()\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count the number of tokens in the input text.\"\"\"\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "# Safe Summarization Function\n",
    "def safe_summarize(content, token_limit=2000):\n",
    "    \"\"\"Summarize content to stay within token limits.\"\"\"\n",
    "    if count_tokens(content) > token_limit:\n",
    "        summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "        return summarizer(content, max_length=300, min_length=50, do_sample=False)[0]['summary_text']\n",
    "    return content\n",
    "\n",
    "# Extract Text from PDFs\n",
    "def extract_text_from_pdfs(pdf_paths):\n",
    "    text = \"\"\n",
    "    for pdf_path in pdf_paths:\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc.load_page(page_num)\n",
    "                text += page.get_text(\"text\") or \"\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "# Generate Embeddings\n",
    "def generate_embeddings(text):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    chunks = split_text_into_chunks(text)\n",
    "    if not chunks:\n",
    "        logging.error(\"No chunks generated for embedding.\")\n",
    "        return [], None\n",
    "    embeddings = model.encode(chunks)\n",
    "    return chunks, embeddings\n",
    "\n",
    "# Create FAISS Index\n",
    "def create_faiss_index(embeddings):\n",
    "    if embeddings.size == 0:\n",
    "        logging.error(\"No embeddings provided to create FAISS index.\")\n",
    "        return None\n",
    "\n",
    "    dim = embeddings.shape[1]  # Dimension of the embeddings\n",
    "    nlist = min(100, embeddings.shape[0])  # Ensure nlist <= number of training points\n",
    "\n",
    "    # Create the FAISS index\n",
    "    quantizer = faiss.IndexFlatL2(dim)  # Quantizer for IVF\n",
    "    index = faiss.IndexIVFFlat(quantizer, dim, nlist)  # Use nlist instead of a fixed value\n",
    "\n",
    "    # Train the index\n",
    "    if embeddings.shape[0] >= nlist:  # Ensure there are enough training points\n",
    "        index.train(embeddings)\n",
    "    else:\n",
    "        logging.warning(f\"Not enough training points ({embeddings.shape[0]}) for {nlist} clusters. Using a flat index instead.\")\n",
    "        index = faiss.IndexFlatL2(dim)  # Fallback to a flat index\n",
    "\n",
    "    # Add embeddings to the index\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# Split Text into Chunks\n",
    "def split_text_into_chunks(text, chunk_size=1000, chunk_overlap=200):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Search for Relevant Content in PDFs\n",
    "def get_top_chunks(query, index, pdf_chunks, top_k=5):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    query_embedding = model.encode([query])\n",
    "    D, I = index.search(query_embedding, k=top_k)\n",
    "    top_chunks = [pdf_chunks[i] for i in I[0] if i < len(pdf_chunks)]\n",
    "    return top_chunks\n",
    "\n",
    "# Dynamic Prompt Builder\n",
    "def build_dynamic_prompt(context, description, num_chunks):\n",
    "    prompt_template = \"\"\"\n",
    "    \"You are a medical expert. Analyze the following skin issue based on {num_chunks} retrieved medical references. Prioritize common skin conditions unless the symptoms strongly suggest a rare or severe disease.\"\n",
    "    Context: {context}\n",
    "    User Description: {description}\n",
    "    Provide the top 5 possible diseases in JSON strictly JSON format:\n",
    "    [\n",
    "        {{\"disease\": \"<Disease Name 1>\", \"score\": <Possibility Score 1>}},\n",
    "        ...\n",
    "    ]\n",
    "    No explanation or additional text should be provided. Only send JSON output.\n",
    "    \"\"\"\n",
    "    human_prompt = HumanMessagePromptTemplate.from_template(prompt_template)\n",
    "    return ChatPromptTemplate.from_messages([human_prompt])\n",
    "\n",
    "# Generate Diagnostic Questions\n",
    "def generate_questions_with_options(disease_list, llm, max_questions=10):\n",
    "    \"\"\"\n",
    "    Generate diagnostic questions with options tailored to the predicted diseases using the LLM.\n",
    "    \"\"\"\n",
    "    # Prepare input for the LLM\n",
    "    prompt = f\"\"\"\n",
    "    You are a medical expert. The following diseases are possible based on user input: {', '.join(disease_list)}.\n",
    "    Generate up to {max_questions} diagnostic multiple-choice questions to help a person distinguish which disease they have among those in this list.\n",
    "    Each question should have 3-4 options, and the options should help differentiate uniquely which disease the patient has.\n",
    "    Questions should be based on user's experience with disease symptoms and not general knowledge questions on disease.\n",
    "    Provide the questions and options in the following JSON format:\n",
    "    [\n",
    "        {{\n",
    "            \"question\": \"Question 1 text\",\n",
    "            \"options\": [\"Option A\", \"Option B\", \"Option C\"],\n",
    "            \"answer\": \"Option A\"\n",
    "        }},\n",
    "        ...\n",
    "    ]\n",
    "    IMPORTANT:\n",
    "    - Do not include any explanations, additional text, or notes.\n",
    "    - Return only the JSON output.\n",
    "    - Ensure the output is valid JSON and can be parsed by a JSON parser.\n",
    "    \"\"\"\n",
    "    # Use the LLM to generate questions\n",
    "    response = llm.invoke(prompt)\n",
    "    questions_json = response[\"content\"].strip() if response else \"\"\n",
    "\n",
    "    logging.info(f\"Questions JSON: {questions_json}\")\n",
    "\n",
    "    # Extract JSON content from the response\n",
    "    try:\n",
    "        # Find the start and end of the JSON content\n",
    "        start = questions_json.find(\"[\")\n",
    "        end = questions_json.rfind(\"]\") + 1\n",
    "\n",
    "        if start == -1 or end == 0:\n",
    "            logging.error(\"No JSON content found in response.\")\n",
    "            return []\n",
    "\n",
    "        # Extract the JSON portion\n",
    "        json_content = questions_json[start:end]\n",
    "\n",
    "        # Parse the JSON\n",
    "        questions = json.loads(json_content)\n",
    "\n",
    "        # Validate the structure of the JSON\n",
    "        if not isinstance(questions, list):\n",
    "            raise ValueError(\"Expected a list of questions.\")\n",
    "\n",
    "        # Ensure each question has the required keys\n",
    "        for question in questions:\n",
    "            if not all(key in question for key in [\"question\", \"options\", \"answer\"]):\n",
    "                raise ValueError(\"Invalid question format in JSON.\")\n",
    "\n",
    "        return questions[:max_questions]\n",
    "\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        logging.error(f\"Failed to parse questions JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "# Refine Disease Prediction\n",
    "def refine_disease_prediction(disease_list, user_answers, llm):\n",
    "    prompt = f\"\"\"\n",
    "    You are a medical expert. Based on the user's symptoms, the following diseases were predicted: {', '.join(disease_list)}.\n",
    "    The user has answered diagnostic questions as follows:\n",
    "    {json.dumps(user_answers, indent=2)}\n",
    "    Use this information to refine the predictions and provide the most likely disease.\n",
    "    Output the final disease in this JSON format:\n",
    "    {{\n",
    "        \"disease\": \"<Final Disease>\",\n",
    "        \"justification\": \"<Reason based on answers>\"\n",
    "    }}\n",
    "    Ensure the output is valid JSON without any additional text.\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    refined_disease_str = response[\"content\"].strip() if response else None\n",
    "    # Attempt to parse JSON\n",
    "    start = refined_disease_str.find(\"{\") if refined_disease_str else -1\n",
    "    end = refined_disease_str.rfind(\"}\") + 1 if refined_disease_str else 0\n",
    "    if start == -1 or end == 0:\n",
    "        logging.error(\"No JSON content found in response.\")\n",
    "        return {\"disease\": \"Unknown\", \"justification\": \"Unable to parse response.\"}\n",
    "    try:\n",
    "        refined_disease = json.loads(refined_disease_str[start:end])\n",
    "    except json.JSONDecodeError as e:\n",
    "        logging.error(f\"Failed to parse JSON response: {e}\")\n",
    "        refined_disease = {\"disease\": \"Unknown\", \"justification\": \"Unable to parse response.\"}\n",
    "    return refined_disease\n",
    "\n",
    "# Fetch Nearby Places\n",
    "def fetch_nearby_places(place_type, user_coordinates, radius=2000):\n",
    "    try:\n",
    "        results = gmaps.places_nearby(location=user_coordinates, radius=radius, type=place_type).get(\"results\", [])\n",
    "        # Additional filtering for skin specialists\n",
    "        if place_type == \"skin specialist\":\n",
    "            keywords = [\"dermatologist\", \"skin\", \"clinic\", \"hospital\"]\n",
    "            results = [place for place in results if any(keyword.lower() in place.get(\"name\", \"\").lower() for keyword in keywords)]\n",
    "        # Extract relevant information\n",
    "        places = []\n",
    "        for place in results:\n",
    "            name = place.get(\"name\", \"Unknown\")\n",
    "            address = place.get(\"vicinity\", \"Address not available\")\n",
    "            rating = place.get(\"rating\", \"No rating available\")\n",
    "            location = place.get(\"geometry\", {}).get(\"location\", {})\n",
    "            if location:\n",
    "                places.append({\n",
    "                    \"name\": name,\n",
    "                    \"address\": address,\n",
    "                    \"rating\": rating,\n",
    "                    \"location\": (location.get(\"lat\"), location.get(\"lng\"))\n",
    "                })\n",
    "            if len(places) >= 5:\n",
    "                break\n",
    "        return places\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Google Places API Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Display Map\n",
    "def display_map(places, user_coordinates):\n",
    "    m = folium.Map(location=user_coordinates, zoom_start=15)\n",
    "    folium.Marker(\n",
    "        location=user_coordinates, popup=\"Your Location\", icon=folium.Icon(color=\"blue\")\n",
    "    ).add_to(m)\n",
    "    for place in places:\n",
    "        folium.Marker(\n",
    "            location=place[\"location\"],\n",
    "            popup=f\"{place['name']} - {place.get('rating', 'No rating')}\\u2b50\",\n",
    "            icon=folium.Icon(color=\"green\")\n",
    "        ).add_to(m)\n",
    "    st_folium(m, width=700, height=500)\n",
    "\n",
    "# Actions Prompt\n",
    "def actions_prompt(context, disease_name, severity):\n",
    "    # Define the prompt template with dynamic placeholders\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a medical expert specializing in providing actionable advice to empower individuals, especially those with limited resources, to make informed decisions about their health. \n",
    "\n",
    "        Disease: {disease_name}\n",
    "        Severity: {severity}\n",
    "        \n",
    "        Instructions:\n",
    "        1. If the severity is **severe**, explain why the condition is critical, emphasize the urgency of consulting a doctor or specialist, and provide immediate actions or precautions to prevent worsening.\n",
    "        2. If the severity is **moderate**, suggest affordable over-the-counter medications (ointments, creams, tablets, etc.), simple home remedies, and actions to manage symptoms effectively.\n",
    "        3. Always include:\n",
    "            - A list of common and affordable home remedies.\n",
    "            - Low-cost, accessible medicines, if applicable.\n",
    "            - Preventive measures to avoid further complications.\n",
    "            - Necessary actions, dos and don’ts, and recovery tips.\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Output the response strictly in the following JSON format:\n",
    "        ```json\n",
    "        {{\n",
    "            \"severity\": \"{severity}\",\n",
    "            \"answer\": {{\n",
    "                \"description\": \"<Brief explanation of the condition>\",\n",
    "                \"immediate_actions\": [\n",
    "                    \"List of actions to take immediately for relief and to prevent worsening.\"\n",
    "                ],\n",
    "                \"home_remedies\": [\n",
    "                    \"Affordable, accessible remedies for symptom relief.\"\n",
    "                ],\n",
    "                \"preventive_measures\": [\n",
    "                    \"Steps to prevent the condition from worsening.\"\n",
    "                ],\n",
    "                \"medications\": [\n",
    "                    \"Over-the-counter medications available at most pharmacies.\"\n",
    "                ],\n",
    "                \"dos_and_donts\": [\n",
    "                    \"Helpful actions to take.\",\n",
    "                    \"Actions to avoid for recovery.\"\n",
    "                ]\n",
    "            }}\n",
    "        }}\n",
    "        ```\n",
    "\n",
    "        Ensure the JSON response is well-formed and adheres strictly to the provided structure. Do not include any extra text outside the JSON object.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Format the prompt with the provided context, disease name, and severity\n",
    "    formatted_prompt = prompt.format_prompt(context=context, disease_name=disease_name, severity=severity).to_string()\n",
    "\n",
    "    return formatted_prompt\n",
    "\n",
    "# Streamlit Interface\n",
    "st.title(\"Skin Disease Diagnosis Assistant\")\n",
    "st.markdown(\"\"\"\n",
    "    🧬 **Diagnose Skin Diseases**: Upload an image, provide a description of the skin problem, and receive guidance on possible diseases along with diagnostic questions for refinement.\n",
    "\"\"\")\n",
    "\n",
    "# User Inputs\n",
    "uploaded_image = st.file_uploader(\"Upload an image of the skin issue (optional):\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "user_description = st.text_area(\"Describe your skin problem (e.g., 'Red rash with itchiness for 3 days'):\")\n",
    "location_input = st.text_input(\"Enter your location (e.g., 'New Delhi, India'):\").strip()\n",
    "\n",
    "# Early validation of description input\n",
    "if not user_description or len(user_description.strip()) < 5:\n",
    "    st.error(\"Please provide a detailed description of your skin problem.\")\n",
    "    st.stop()  # Stop further processing if input is invalid\n",
    "\n",
    "# Initialize a list to hold disease predictions\n",
    "disease_predictions = []\n",
    "\n",
    "# Load the image model\n",
    "IMAGE_MODEL_PATH = 'skin_disease_model.pth'\n",
    "\n",
    "# Define the model architecture if needed (Uncomment and modify if using state_dict)\n",
    "# from torchvision import models\n",
    "\n",
    "# class SkinDiseaseModel(torch.nn.Module):\n",
    "#     def __init__(self, num_classes=10):\n",
    "#         super(SkinDiseaseModel, self).__init__()\n",
    "#         self.model = models.resnet50(pretrained=False)\n",
    "#         num_features = self.model.fc.in_features\n",
    "#         self.model.fc = torch.nn.Linear(num_features, num_classes)  # Adjust num_classes\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# try:\n",
    "#     image_model = SkinDiseaseModel(num_classes=10)  # Adjust num_classes accordingly\n",
    "#     image_model.load_state_dict(torch.load(IMAGE_MODEL_PATH, map_location=torch.device('cpu')))\n",
    "#     image_model.eval()\n",
    "#     logging.info(\"Image model loaded successfully from state dictionary.\")\n",
    "# except Exception as e:\n",
    "#     logging.error(f\"Error loading image model: {e}\")\n",
    "#     st.error(\"Failed to load the image model. Please check the model file and architecture.\")\n",
    "\n",
    "# Alternatively, if loading the complete model\n",
    "try:\n",
    "    image_model = torch.load(IMAGE_MODEL_PATH, map_location=torch.device('cpu'))\n",
    "    image_model.eval()\n",
    "    logging.info(\"Image model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading image model: {e}\")\n",
    "    st.error(\"Failed to load the image model. Please check the model file.\")\n",
    "\n",
    "# Define image transformations (modify based on your model's training)\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Adjust size as per model's input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],  # Mean values\n",
    "                         [0.229, 0.224, 0.225])  # Std deviation\n",
    "])\n",
    "\n",
    "def map_disease_label(index):\n",
    "    \"\"\"\n",
    "    Maps a class index to the corresponding disease name.\n",
    "    Update this mapping based on your model's training labels.\n",
    "    \"\"\"\n",
    "    disease_labels = {\n",
    "        0: 'Acne',\n",
    "        1: 'Eczema',\n",
    "        2: 'Psoriasis',\n",
    "        3: 'Rosacea',\n",
    "        4: 'Vitiligo',\n",
    "        5: 'Melanoma',\n",
    "        6: 'Dermatitis',\n",
    "        7: 'Hives',\n",
    "        8: 'Warts',\n",
    "        9: 'Other'\n",
    "        # Add all necessary mappings\n",
    "    }\n",
    "    return disease_labels.get(index, 'Unknown Disease')\n",
    "\n",
    "def predict_disease_from_image(image_file):\n",
    "    try:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        image_tensor = image_transforms(image).unsqueeze(0)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            outputs = image_model(image_tensor)\n",
    "            # If the model outputs logits\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            _, predicted_idx = torch.max(probabilities, 1)\n",
    "            predicted_disease = map_disease_label(predicted_idx.item())\n",
    "            return predicted_disease\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during image prediction: {e}\")\n",
    "        return \"Prediction Failed\"\n",
    "\n",
    "# Process image input if uploaded\n",
    "if uploaded_image is not None:\n",
    "    st.image(uploaded_image, caption='Uploaded Image', use_column_width=True)\n",
    "    st.info(\"Processing image...\")\n",
    "    image_pred = predict_disease_from_image(uploaded_image)\n",
    "    st.success(f\"Image-based Prediction: {image_pred}\")\n",
    "    disease_predictions.append(image_pred)\n",
    "else:\n",
    "    st.warning(\"No image uploaded. Proceeding with text-based predictions only.\")\n",
    "\n",
    "try:\n",
    "    # Extract and Process PDF Data\n",
    "    pdf_paths = [\"skin_diseases_guide1.pdf\", \"skin_diseases_guide2.pdf\"]\n",
    "\n",
    "    def clean_pdf_text(text):\n",
    "        \"\"\"Remove irrelevant sections like acknowledgments, prefaces, and exam questions.\"\"\"\n",
    "        filtered_text = [line.strip() for line in text.split(\"\\n\") if not any(keyword in line for keyword in [\"ACKNOWLEDGEMENT\", \"PREFACE\", \"Table\", \"exam\"])]\n",
    "        return \" \".join(filtered_text)\n",
    "\n",
    "    # Extract text from PDFs\n",
    "    pdf_text = extract_text_from_pdfs(pdf_paths)\n",
    "    if not pdf_text:\n",
    "        st.error(\"Failed to extract text from provided PDF files.\")\n",
    "        st.stop()\n",
    "\n",
    "    logging.info(\"PDF text extracted successfully.\")\n",
    "    pdf_text = clean_pdf_text(pdf_text)\n",
    "    logging.info(\"PDF text cleaned successfully.\")\n",
    "    \n",
    "    # Generate embeddings and create index\n",
    "    pdf_chunks, pdf_embeddings = generate_embeddings(pdf_text)\n",
    "    logging.info(\"Embeddings generated successfully.\")\n",
    "    pdf_index = create_faiss_index(pdf_embeddings)\n",
    "    \n",
    "    if not pdf_index:\n",
    "        st.error(\"Failed to create FAISS index. Ensure embeddings were generated correctly.\")\n",
    "        st.stop()\n",
    "\n",
    "    logging.info(\"FAISS index created successfully.\")\n",
    "\n",
    "    # Search for relevant content based on user description\n",
    "    pdf_search_results = get_top_chunks(user_description, pdf_index, pdf_chunks, top_k=3)\n",
    "    context = safe_summarize(\" \".join(pdf_search_results), token_limit=3000)\n",
    "\n",
    "    num_chunks = min(len(pdf_search_results), 5)\n",
    "    prompt = build_dynamic_prompt(context, user_description, num_chunks)\n",
    "    query_context = {\"context\": context, \"description\": user_description, \"num_chunks\": num_chunks}\n",
    "    formatted_prompt = prompt.format_prompt(**query_context).to_string()\n",
    "\n",
    "    logging.info(f\"Final prompt for LLM:\\n{formatted_prompt}\")\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "\n",
    "    if not response or not response[\"content\"].strip():\n",
    "        st.error(\"Unable to process diagnosis. Please try again.\")\n",
    "        st.stop()\n",
    "\n",
    "    raw_response = response[\"content\"].strip()\n",
    "    logging.info(f\"Raw LLM response: {raw_response}\")\n",
    "\n",
    "    # Parsing the response\n",
    "    try:\n",
    "        diseases_with_scores = json.loads(raw_response)\n",
    "        logging.info(f\"LLM returned diseases with scores: {diseases_with_scores}\")\n",
    "    except json.JSONDecodeError:\n",
    "        logging.error(\"Failed to parse JSON response.\")\n",
    "        st.error(\"Unable to process the diseases with scores. Please try again.\")\n",
    "        st.stop()\n",
    "\n",
    "    if diseases_with_scores:\n",
    "        st.markdown(\"### Possible Diseases with Scores:\")\n",
    "        st.json(diseases_with_scores)\n",
    "\n",
    "        # Extract disease names from text-based predictions\n",
    "        text_diseases = [disease[\"disease\"] for disease in diseases_with_scores]\n",
    "        disease_predictions.extend(text_diseases)  # Combine image and text predictions\n",
    "\n",
    "        # Limit to top 6 diseases (1 image + 5 text)\n",
    "        disease_predictions = disease_predictions[:6]\n",
    "\n",
    "        st.markdown(\"### Combined Disease Predictions:\")\n",
    "        st.json(disease_predictions)\n",
    "\n",
    "        # Generate diagnostic questions based on combined predictions\n",
    "        diagnostic_questions = generate_questions_with_options(disease_predictions, llm, max_questions=10)\n",
    "\n",
    "        if diagnostic_questions:\n",
    "            st.markdown(\"### Diagnostic Questions:\")\n",
    "\n",
    "            # Initialize session state for user answers if not already present\n",
    "            if \"user_answers\" not in st.session_state:\n",
    "                st.session_state.user_answers = {}\n",
    "\n",
    "            for question in diagnostic_questions:\n",
    "                if question[\"question\"] not in st.session_state.user_answers:\n",
    "                    st.session_state.user_answers[question[\"question\"]] = st.selectbox(\n",
    "                        question[\"question\"],\n",
    "                        question[\"options\"],\n",
    "                        key=question[\"question\"]\n",
    "                    )\n",
    "\n",
    "            if st.button(\"Submit Answers\"):\n",
    "                st.success(\"Answers submitted successfully!\")\n",
    "                refined_prediction = refine_disease_prediction(disease_predictions, st.session_state.user_answers, llm)\n",
    "                st.markdown(\"### Final Disease Prediction:\")\n",
    "                st.json(refined_prediction)\n",
    "\n",
    "                if isinstance(refined_prediction, dict) and \"disease\" in refined_prediction:\n",
    "                    disease_name = refined_prediction[\"disease\"]\n",
    "                    st.info(f\"Stored final disease: {disease_name}\")\n",
    "                else:\n",
    "                    st.warning(\"Could not extract a valid disease name from the refined prediction.\")\n",
    "                    disease_name = \"\"\n",
    "        else:\n",
    "            st.warning(\"No diagnostic questions generated. Please try again with more detailed input.\")\n",
    "    else:\n",
    "        st.warning(\"No diseases were identified. Please try again with more detailed input.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {e}\")\n",
    "    st.error(f\"An error occurred while processing your request: {e}\")\n",
    "    st.stop()\n",
    "\n",
    "# Additional handling for disease remedies and location if disease_name is found\n",
    "if 'disease_name' in locals() and disease_name:\n",
    "    try:\n",
    "        # Fetch remedies related to the disease\n",
    "        pdf_paths = [\"remedies1.pdf\", \"remedies2.pdf\"]\n",
    "        pdf_text = clean_pdf_text(extract_text_from_pdfs(pdf_paths))\n",
    "        pdf_chunks, pdf_embeddings = generate_embeddings(pdf_text)\n",
    "        pdf_index = create_faiss_index(pdf_embeddings)\n",
    "\n",
    "        # Query the PDF for remedies based on the disease name\n",
    "        context = \" \".join(get_top_chunks(disease_name, pdf_index, pdf_chunks))\n",
    "\n",
    "        # Severity assessment\n",
    "        severity_prompt = f\"\"\"\n",
    "        Classify the disease {disease_name} in terms of its effects as one of the following: mild, moderate, or severe. \n",
    "        Only return one string as output: \"mild\", \"moderate\", or \"severe\". \n",
    "        Do not provide any explanations or additional text.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            severity_response = llm.invoke(severity_prompt)\n",
    "            severity = severity_response[\"content\"].strip().lower() if severity_response else None\n",
    "            logging.info(f\"Severity returned: {severity}\")\n",
    "\n",
    "            if severity not in [\"mild\", \"moderate\", \"severe\"]:\n",
    "                raise ValueError(f\"Invalid severity level returned: {severity}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during severity assessment: {e}\")\n",
    "            st.error(\"An error occurred while assessing severity. Please try again.\")\n",
    "            severity = None\n",
    "\n",
    "        if severity:\n",
    "            # Generate the prompt for actionable advice\n",
    "            prompt = actions_prompt(context, disease_name, severity)\n",
    "            response = llm.invoke(prompt)\n",
    "            logging.info(f\"Model response: {response['content']}\")  # Log the raw response\n",
    "\n",
    "            # Attempt to parse the JSON from the model's response\n",
    "            try:\n",
    "                # Extract content between \"```json\" and \"```\" if delimiters are used\n",
    "                if \"```json\" in response[\"content\"]:\n",
    "                    json_str = response[\"content\"].split(\"```json\")[1].strip().rstrip(\"```\").strip()\n",
    "                elif response[\"content\"].strip().startswith(\"{\") and response[\"content\"].strip().endswith(\"}\"):\n",
    "                    json_str = response[\"content\"].strip()\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid JSON format in response.\")\n",
    "\n",
    "                # Parse the JSON\n",
    "                remedies = json.loads(json_str)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error parsing JSON from response: {e}\")\n",
    "                st.error(\"An error occurred while parsing the recommendation. Please try again.\")\n",
    "                remedies = None\n",
    "\n",
    "            # Display Remedies\n",
    "            if remedies:\n",
    "                st.subheader(\"Answer:\")\n",
    "                st.json(remedies)\n",
    "            else:\n",
    "                st.warning(\"No recommendations available at this time.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred: {e}\")\n",
    "        st.error(f\"An error occurred while processing remedies: {e}\")\n",
    "\n",
    "# Handling user location and nearby services\n",
    "if location_input:\n",
    "    try:\n",
    "        geolocator = Nominatim(user_agent=\"skingenie\")\n",
    "        user_location = geolocator.geocode(location_input)\n",
    "\n",
    "        if user_location:\n",
    "            user_coordinates = (user_location.latitude, user_location.longitude)\n",
    "            nearby_places_pharmacy = fetch_nearby_places(\"pharmacy\", user_coordinates)\n",
    "            nearby_places_skin_specialist = fetch_nearby_places(\"skin specialist\", user_coordinates)\n",
    "\n",
    "            if nearby_places_pharmacy:\n",
    "                st.subheader(\"Nearby Pharmacies:\")\n",
    "                for place in nearby_places_pharmacy:\n",
    "                    st.write(f\"**{place['name']}** - {place['address']} ({place['rating']}⭐)\")\n",
    "\n",
    "            if nearby_places_skin_specialist:\n",
    "                st.subheader(\"Nearby Skin Specialists:\")\n",
    "                for place in nearby_places_skin_specialist:\n",
    "                    st.write(f\"**{place['name']}** - {place['address']} ({place['rating']}⭐)\")\n",
    "\n",
    "            if nearby_places_pharmacy or nearby_places_skin_specialist:\n",
    "                display_map(nearby_places_pharmacy + nearby_places_skin_specialist, user_coordinates)\n",
    "            else:\n",
    "                st.warning(\"No nearby locations found.\")\n",
    "        else:\n",
    "            st.error(\"Invalid location. Please try entering a different location.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred: {e}\")\n",
    "        st.error(f\"An error occurred while processing the location: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
